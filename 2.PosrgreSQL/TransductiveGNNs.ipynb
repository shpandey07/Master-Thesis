{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3b3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: py4j in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (1.7.1)\n",
      "Requirement already satisfied: future in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (2.6.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (4.62.3)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from tqdm->hyperopt) (0.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (2.26.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (3.10.10)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (5.8.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (3.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (1.21.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from torch_geometric) (2021.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (1.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from jinja2->torch_geometric) (1.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->torch_geometric) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->torch_geometric) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from tqdm->torch_geometric) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt\n",
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87003c47-b14b-4138-a8c5-727a0ac77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, ChebConv, SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d282aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(nodes_count, edges_count):\n",
    "        \n",
    "    # Import required libraries\n",
    "    import psycopg2\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    import re\n",
    "    import time\n",
    "    from psycopg2 import sql\n",
    "\n",
    "    # Helper function to connect to SQL database\n",
    "    def sql_connector(dataset_name):\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "\n",
    "        return connection\n",
    "\n",
    "    # Function to extract number of noes and edges from the file name\n",
    "    def extract_file_info(file_name):\n",
    "        \"\"\"\n",
    "        Extracts the number of nodes and edges from the filename.\n",
    "\n",
    "        The filename is expected to be in the format `X_<num_nodes>_nodes_<num_edges>_edges.csv`.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): Path to the file with encoded information on nodes and edges.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two integers:\n",
    "                - num_nodes (int): The number of nodes extracted from the filename.\n",
    "                - num_edges (int): The number of edges extracted from the filename.\n",
    "        \"\"\"\n",
    "\n",
    "        match = re.search(r'(\\d+)_nodes_(\\d+)_edges', file_name)\n",
    "        if match:\n",
    "            num_nodes = int(match.group(1))\n",
    "            num_edges = int(match.group(2))\n",
    "            return num_nodes, num_edges\n",
    "        else:\n",
    "            raise ValueError(f\"Filename format is not recognized: {file_name}\")\n",
    "\n",
    "    # Function to create a database\n",
    "    def create_database(db_name):\n",
    "        \"\"\"\n",
    "        Creates a new PostgreSQL database.\n",
    "\n",
    "        Parameters:\n",
    "        db_name (str): The name of the database to be created.\n",
    "        \"\"\"\n",
    "\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='postgres',  # Connect to default db to create new db\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        connection.autocommit = True\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        try:\n",
    "            # Create the database\n",
    "            cursor.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(db_name)))\n",
    "            print(f\"Database {db_name} created successfully.\")\n",
    "            \n",
    "            # Connect to the newly created database and enable pg_stat_statements\n",
    "            new_db_connection = psycopg2.connect(\n",
    "                dbname=db_name,\n",
    "                user='postgres',\n",
    "                password='Berlin!321',\n",
    "                host='localhost'\n",
    "            )\n",
    "            new_db_cursor = new_db_connection.cursor()\n",
    "            \n",
    "            new_db_cursor.close()\n",
    "            new_db_connection.close()\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    # Function to create tables in the PostgreSQL database with indexing and track index creation times\n",
    "    def create_tables_with_index(connection):\n",
    "        \"\"\"\n",
    "        Creates the tables 'nodes', 'edges', and 'labels' in the connected PostgreSQL database, if they do not exist.\n",
    "        Also adds indexes on certain columns to improve query performance.\n",
    "\n",
    "        Parameters:\n",
    "            connection (PostgreSQL connection object): Connection to the PostgreSQL database.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two values:\n",
    "                - mean_data_loading_index_time (float): Mean execution time for creating the indexes.\n",
    "                - std_data_loading_index_time (float): Standard deviation of the execution time for creating the indexes.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Function to execute an index creation and track the time\n",
    "        def create_index(query):\n",
    "            cursor.execute(query)     # Execute index creation\n",
    "            connection.commit()       # Commit after each index creation\n",
    "        \n",
    "        # Create the edges table and its indexes\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS edges (\n",
    "                source INT, \n",
    "                target INT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes and track their creation times\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS source_index ON edges (source)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS target_index ON edges (target)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS source_target_index ON edges (source, target)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS target_source_index ON edges (target, source)\")\n",
    "\n",
    "        # Create the nodes table and its features\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS nodes (node_id SERIAL PRIMARY KEY)\")\n",
    "        \n",
    "        # Dynamically add features if they do not exist\n",
    "        cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='nodes'\")\n",
    "        columns = [column[0] for column in cursor.fetchall()]\n",
    "        \n",
    "        for i in range(100):  # Assuming 100 features\n",
    "            feature_name = f\"feature_{i}\"\n",
    "            if feature_name not in columns:\n",
    "                cursor.execute(f\"ALTER TABLE nodes ADD COLUMN {feature_name} FLOAT\")\n",
    "\n",
    "        # Create the labels table and its index\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS labels (\n",
    "                node_id INT REFERENCES nodes(node_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS node_id_index ON labels (node_id)\")\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    # Function to load edges data into PostgreSQL\n",
    "    def load_edges(connection, file_path):\n",
    "        \"\"\"\n",
    "        Loads edges from a CSV file into the edges table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        edge_file (str): The path to the CSV file containing edge data.\n",
    "        \"\"\"\n",
    "        print(\"Edges are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as edges_file:\n",
    "            edges_reader = csv.reader(edges_file)\n",
    "            next(edges_reader)  # Skip header\n",
    "\n",
    "            # Prepare a multi-row insert statement\n",
    "            args_str = ','.join(cursor.mogrify(\"(%s,%s)\", (int(row[0]), int(row[1]))).decode(\"utf-8\") \n",
    "                                            for row in edges_reader)\n",
    "            query = f\"INSERT INTO edges (source, target) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Edge loading completed!\")\n",
    "\n",
    "    # Function to load nodes data into PostgreSQL \n",
    "    def load_nodes(connection, file_path):\n",
    "        \"\"\"\n",
    "        Loads nodes from a CSV file into the nodes table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        node_file (str): The path to the CSV file containing node data.\n",
    "        \"\"\"\n",
    "        print(\"Nodes are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as nodes_file:\n",
    "            nodes_reader = csv.reader(nodes_file)\n",
    "            next(nodes_reader)  # Skip header\n",
    "\n",
    "            # Prepare a multi-row insert statement\n",
    "            args_str = ','.join(\n",
    "                cursor.mogrify(\"(\" + \",\".join([\"%s\"] * 100) + \")\", tuple(map(float, row))).decode(\"utf-8\")\n",
    "                for row in nodes_reader\n",
    "            )\n",
    "            query = f\"INSERT INTO nodes ({', '.join([f'feature_{i}' for i in range(100)])}) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Node loading completed!\")\n",
    "\n",
    "    # Function to load labels data into PostgreSQL\n",
    "    def load_labels(connection, file_path):\n",
    "        \"\"\"\n",
    "        Loads labels from a CSV file into the labels table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        label_file (str): The path to the CSV file containing label data.\n",
    "        \"\"\"\n",
    "        print(\"Labels are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Ensure the 'label' column exists\n",
    "        cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='labels'\")\n",
    "        existing_columns = set([column[0] for column in cursor.fetchall()])\n",
    "        if \"label\" not in existing_columns:\n",
    "            cursor.execute(\"ALTER TABLE labels ADD COLUMN label TEXT\")\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as labels_file:\n",
    "            labels_reader = csv.reader(labels_file)\n",
    "            next(labels_reader)  # Skip header\n",
    "\n",
    "            # Prepare a multi-row insert statement\n",
    "            args_str = ','.join(cursor.mogrify(\"(%s,%s)\", (idx, row[0])).decode(\"utf-8\")\n",
    "                                for idx, row in enumerate(labels_reader, start=1))\n",
    "            query = f\"INSERT INTO labels (node_id, label) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Label loading completed!\")\n",
    "    \n",
    "    # Function to get the node table using PostgreSQL    \n",
    "    def get_node_table_from_sql(dataset_name) -> np.array:  # Change the return type to np.array\n",
    "        \"\"\"\n",
    "        Connects to the database, retrieves node features (excluding node_id) from the nodes table, \n",
    "        and returns them as a numpy array.\n",
    "        \n",
    "        Parameters:\n",
    "        dataset_name (str): The name of the dataset (i.e., the database name).\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: A 2D numpy array of node features.\n",
    "        \"\"\"\n",
    "        # Connect to the PostgreSQL database\n",
    "        connector = sql_connector(dataset_name)\n",
    "        \n",
    "        # Create a cursor from the connection\n",
    "        cursor = connector.cursor()\n",
    "        \n",
    "        # Execute the SQL query\n",
    "        cursor.execute(\"SELECT * FROM nodes\")  # Select all columns\n",
    "        node_data = cursor.fetchall()  # Fetch all the rows returned by the query\n",
    "        \n",
    "        # Close the cursor\n",
    "        cursor.close()\n",
    "\n",
    "        # Convert the data to a numpy array\n",
    "        node_data = np.array(node_data)\n",
    "\n",
    "        # Debugging: Print the shape of the node_data\n",
    "        print(\"Fetched node data shape:\", node_data.shape)\n",
    "\n",
    "        # Ensure the node data contains the correct number of feature columns\n",
    "        if node_data.size == 0:  # Check if no data is returned\n",
    "            raise ValueError(\"No data returned from the nodes table.\")\n",
    "\n",
    "        if node_data.shape[1] != 101:  # 100 features + 1 node_id column\n",
    "            raise ValueError(f\"Unexpected data shape. Ensure node table contains exactly 100 feature columns plus the node_id. Current shape: {node_data.shape}.\")\n",
    "\n",
    "        # Extract features excluding the node_id column\n",
    "        node_features = node_data[:, 1:]  # This gets all columns except the first one (node_id)\n",
    "\n",
    "        return node_features  # Return only the features\n",
    "\n",
    "\n",
    "    # Function to get the edge index table using PostgreSQL\n",
    "    def get_edge_table_from_sql(connection):\n",
    "        \"\"\"\n",
    "        Retrieves the edge table from the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "\n",
    "        Returns:\n",
    "        list: List of edges as tuples.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT * FROM edges\")\n",
    "        edge_data = cursor.fetchall()\n",
    "\n",
    "        if not edge_data:\n",
    "            raise ValueError(\"No data returned from the edges table.\")\n",
    "\n",
    "        return edge_data\n",
    "\n",
    "    #  Main block\n",
    "    if __name__ == \"__main__\":\n",
    "        \"\"\"\n",
    "        Main execution block of the script.\n",
    "\n",
    "        This block initializes the parameters, creates the database and tables,\n",
    "        loads data from CSV files, measures performance for various operations,\n",
    "        and logs the results.\n",
    "\n",
    "        The following operations are performed:\n",
    "        - Loading edges, nodes, and labels.\n",
    "        - Reading data with a specified number of hops.\n",
    "        - Updating node features and edge weights.\n",
    "        - Deleting all data.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Define file paths using\n",
    "        node_file = f'E:/Master Thesis/Thesis_Code/data/X_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "        edge_file = f'E:/Master Thesis/Thesis_Code/data/edge_index_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "        label_file = f'E:/Master Thesis/Thesis_Code/data/y_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "  \n",
    "\n",
    "        nodes_count, edges_count = extract_file_info(node_file)\n",
    "        db_name = f\"db_{nodes_count}_nodes_{edges_count}_edges\"\n",
    "\n",
    "        # Create a new database\n",
    "        create_database(db_name)    \n",
    "\n",
    "        # Connect to the newly created database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        \n",
    "        # Create tables with indexing\n",
    "        create_tables_with_index(connection)\n",
    "\n",
    "        # Load nodes, edges, and labels\n",
    "        load_nodes(connection, node_file)\n",
    "        load_edges(connection, edge_file)\n",
    "        load_labels(connection, label_file)\n",
    "    \n",
    "        # Get node features and edge table\n",
    "        node_features = get_node_table_from_sql(connection)\n",
    "        print(\"Node Features Shape:\", node_features.shape)\n",
    "\n",
    "        # Load the labels from the labels table if needed\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT label FROM labels\")\n",
    "        labels_data = cursor.fetchall()\n",
    "\n",
    "        # Extract labels from fetched data\n",
    "        # node_labels = np.array([label[0] for label in labels_data])  # Assuming label is in the first column\n",
    "        node_labels = np.array([int(label[0]) for label in labels_data])  # Use float() if needed\n",
    "        print(\"Node Labels Shape:\", node_labels.shape)\n",
    "\n",
    "        edge_data = get_edge_table_from_sql(connection)\n",
    "        edge_index = np.array(edge_data, dtype=np.int64).T  # Ensure edge_index is of type int64\n",
    "        print(\"Edge Data Length:\", len(edge_index))\n",
    "\n",
    "        # Close the connection\n",
    "        connection.close()\n",
    "\n",
    "        \n",
    "        return node_features, node_labels, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f101f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_batches(nodes_count, edges_count):        \n",
    "    # Import required libraries\n",
    "    import psycopg2\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    import re\n",
    "    import time\n",
    "    from psycopg2 import sql\n",
    "\n",
    "    # Helper function to connect to SQL database\n",
    "    def sql_connector(dataset_name):\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "\n",
    "        return connection\n",
    "\n",
    "    # Function to extract number of noes and edges from the file name\n",
    "    def extract_file_info(file_name):\n",
    "        \"\"\"\n",
    "        Extracts the number of nodes and edges from the filename.\n",
    "\n",
    "        The filename is expected to be in the format `X_<num_nodes>_nodes_<num_edges>_edges.csv`.\n",
    "\n",
    "        Parameters:\n",
    "            filename (str): Path to the file with encoded information on nodes and edges.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two integers:\n",
    "                - num_nodes (int): The number of nodes extracted from the filename.\n",
    "                - num_edges (int): The number of edges extracted from the filename.\n",
    "        \"\"\"\n",
    "\n",
    "        match = re.search(r'(\\d+)_nodes_(\\d+)_edges', file_name)\n",
    "        if match:\n",
    "            num_nodes = int(match.group(1))\n",
    "            num_edges = int(match.group(2))\n",
    "            return num_nodes, num_edges\n",
    "        else:\n",
    "            raise ValueError(f\"Filename format is not recognized: {file_name}\")\n",
    "\n",
    "    # Function to create a database\n",
    "    def create_database(db_name):\n",
    "        \"\"\"\n",
    "        Creates a new PostgreSQL database.\n",
    "\n",
    "        Parameters:\n",
    "        db_name (str): The name of the database to be created.\n",
    "        \"\"\"\n",
    "\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='postgres',  # Connect to default db to create new db\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        connection.autocommit = True\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        try:\n",
    "            # Create the database\n",
    "            cursor.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(db_name)))\n",
    "            print(f\"Database {db_name} created successfully.\")\n",
    "            \n",
    "            # Connect to the newly created database and enable pg_stat_statements\n",
    "            new_db_connection = psycopg2.connect(\n",
    "                dbname=db_name,\n",
    "                user='postgres',\n",
    "                password='Berlin!321',\n",
    "                host='localhost'\n",
    "            )\n",
    "            new_db_cursor = new_db_connection.cursor()\n",
    "            \n",
    "            new_db_cursor.close()\n",
    "            new_db_connection.close()\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    # Function to create tables in the PostgreSQL database with indexing and track index creation times\n",
    "    def create_tables_with_index(connection):\n",
    "        \"\"\"\n",
    "        Creates the tables 'nodes', 'edges', and 'labels' in the connected PostgreSQL database, if they do not exist.\n",
    "        Also adds indexes on certain columns to improve query performance.\n",
    "\n",
    "        Parameters:\n",
    "            connection (PostgreSQL connection object): Connection to the PostgreSQL database.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two values:\n",
    "                - mean_data_loading_index_time (float): Mean execution time for creating the indexes.\n",
    "                - std_data_loading_index_time (float): Standard deviation of the execution time for creating the indexes.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Function to execute an index creation and track the time\n",
    "        def create_index(query):\n",
    "            cursor.execute(query)     # Execute index creation\n",
    "            connection.commit()       # Commit after each index creation\n",
    "        \n",
    "        # Create the edges table and its indexes\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS edges (\n",
    "                source INT, \n",
    "                target INT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes and track their creation times\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS source_index ON edges (source)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS target_index ON edges (target)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS source_target_index ON edges (source, target)\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS target_source_index ON edges (target, source)\")\n",
    "\n",
    "        # Create the nodes table and its features\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS nodes (node_id SERIAL PRIMARY KEY)\")\n",
    "        \n",
    "        # Dynamically add features if they do not exist\n",
    "        cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='nodes'\")\n",
    "        columns = [column[0] for column in cursor.fetchall()]\n",
    "        \n",
    "        for i in range(100):  # Assuming 100 features\n",
    "            feature_name = f\"feature_{i}\"\n",
    "            if feature_name not in columns:\n",
    "                cursor.execute(f\"ALTER TABLE nodes ADD COLUMN {feature_name} FLOAT\")\n",
    "\n",
    "        # Create the labels table and its index\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS labels (\n",
    "                node_id INT REFERENCES nodes(node_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        create_index(\"CREATE INDEX IF NOT EXISTS node_id_index ON labels (node_id)\")\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "        # Function to load edges data into PostgreSQL\n",
    "    \n",
    "    def load_edges(connection, file_path, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Loads edges from a CSV file into the edges table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        edge_file (str): The path to the CSV file containing edge data.\n",
    "        \"\"\"\n",
    "        print(\"Edges are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as edges_file:\n",
    "            edges_reader = csv.reader(edges_file)\n",
    "            next(edges_reader)  # Skip header\n",
    "\n",
    "            batch = []\n",
    "            total_execution_time = 0.0\n",
    "\n",
    "            for row in edges_reader:\n",
    "                batch.append((int(row[0]), int(row[1])))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\") for row in batch)\n",
    "                    query = f\"INSERT INTO edges (source, target) VALUES {args_str};\"\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "\n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\") for row in batch)\n",
    "                query = f\"INSERT INTO edges (source, target) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Edge loading completed!\")\n",
    "\n",
    "    # Function to load nodes data into PostgreSQL \n",
    "    def load_nodes(connection, file_path, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Loads nodes from a CSV file into the nodes table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        node_file (str): The path to the CSV file containing node data.\n",
    "        \"\"\"\n",
    "        print(\"Nodes are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as nodes_file:\n",
    "            nodes_reader = csv.reader(nodes_file)\n",
    "            next(nodes_reader)  # Skip header\n",
    "\n",
    "            batch = []\n",
    "            total_execution_time = 0.0\n",
    "\n",
    "            for row in nodes_reader:\n",
    "                batch.append(tuple(map(float, row)))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(\n",
    "                        cursor.mogrify(\"(\" + \",\".join([\"%s\"] * 100) + \")\", row).decode(\"utf-8\")\n",
    "                        for row in batch\n",
    "                    )\n",
    "                    query = f\"INSERT INTO nodes ({', '.join([f'feature_{i}' for i in range(100)])}) VALUES {args_str};\"\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "\n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(\n",
    "                    cursor.mogrify(\"(\" + \",\".join([\"%s\"] * 100) + \")\", row).decode(\"utf-8\")\n",
    "                    for row in batch\n",
    "                )\n",
    "                query = f\"INSERT INTO nodes ({', '.join([f'feature_{i}' for i in range(100)])}) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Node loading completed!\")\n",
    "\n",
    "    # Function to load labels data into PostgreSQL\n",
    "    def load_labels(connection, file_path, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Loads labels from a CSV file into the labels table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        label_file (str): The path to the CSV file containing label data.\n",
    "        \"\"\"\n",
    "        print(\"Labels are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Ensure the 'label' column exists\n",
    "        cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='labels'\")\n",
    "        existing_columns = set([column[0] for column in cursor.fetchall()])\n",
    "        if \"label\" not in existing_columns:\n",
    "            cursor.execute(\"ALTER TABLE labels ADD COLUMN label TEXT\")\n",
    "        \n",
    "        # Read all rows from the CSV file\n",
    "        with open(file_path, 'r') as labels_file:\n",
    "            labels_reader = csv.reader(labels_file)\n",
    "            next(labels_reader)  # Skip header\n",
    "\n",
    "            batch = []\n",
    "            total_execution_time = 0.0\n",
    "\n",
    "            for idx, row in enumerate(labels_reader, start=1):\n",
    "                batch.append((idx, row[0]))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\")\n",
    "                                        for row in batch)\n",
    "                    query = f\"INSERT INTO labels (node_id, label) VALUES {args_str};\"\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "\n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\")\n",
    "                                    for row in batch)\n",
    "                query = f\"INSERT INTO labels (node_id, label) VALUES {args_str};\"\n",
    "\n",
    "            # Execute the query\n",
    "            cursor.execute(query)\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Label loading completed!\")\n",
    "        \n",
    "    # Function to get the node table using PostgreSQL    \n",
    "    def get_node_table_from_sql(dataset_name) -> np.array:  \n",
    "        \"\"\"\n",
    "        Connects to the database, retrieves node features (excluding node_id) from the nodes table, \n",
    "        and returns them as a numpy array.\n",
    "        \n",
    "        Parameters:\n",
    "        dataset_name (str): The name of the dataset (i.e., the database name).\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: A 2D numpy array of node features.\n",
    "        \"\"\"\n",
    "        # Connect to the PostgreSQL database\n",
    "        connector = sql_connector(dataset_name)\n",
    "        \n",
    "        # Create a cursor from the connection\n",
    "        cursor = connector.cursor()\n",
    "        \n",
    "        # Execute the SQL query\n",
    "        cursor.execute(\"SELECT * FROM nodes\")  # Select all columns\n",
    "        node_data = cursor.fetchall()  # Fetch all the rows returned by the query\n",
    "        \n",
    "        # Close the cursor\n",
    "        cursor.close()\n",
    "\n",
    "        # Convert the data to a numpy array\n",
    "        node_data = np.array(node_data)\n",
    "\n",
    "        # Debugging: Print the shape of the node_data\n",
    "        print(\"Fetched node data shape:\", node_data.shape)\n",
    "\n",
    "        # Ensure the node data contains the correct number of feature columns\n",
    "        if node_data.size == 0:  # Check if no data is returned\n",
    "            raise ValueError(\"No data returned from the nodes table.\")\n",
    "\n",
    "        if node_data.shape[1] != 101:  # 100 features + 1 node_id column\n",
    "            raise ValueError(f\"Unexpected data shape. Ensure node table contains exactly 100 feature columns plus the node_id. Current shape: {node_data.shape}.\")\n",
    "\n",
    "        # Extract features excluding the node_id column\n",
    "        node_features = node_data[:, 1:]  # This gets all columns except the first one (node_id)\n",
    "\n",
    "        return node_features  # Return only the features\n",
    "\n",
    "    # Function to get the edge index table using PostgreSQL\n",
    "    def get_edge_table_from_sql(connection):\n",
    "        \"\"\"\n",
    "        Retrieves the edge table from the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "\n",
    "        Returns:\n",
    "        list: List of edges as tuples.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT * FROM edges\")\n",
    "        edge_data = cursor.fetchall()\n",
    "\n",
    "        if not edge_data:\n",
    "            raise ValueError(\"No data returned from the edges table.\")\n",
    "\n",
    "        return edge_data\n",
    "\n",
    "    #  Main block\n",
    "    if __name__ == \"__main__\":\n",
    "        \"\"\"\n",
    "        Main execution block of the script.\n",
    "\n",
    "        This block initializes the parameters, creates the database and tables,\n",
    "        loads data from CSV files, measures performance for various operations,\n",
    "        and logs the results.\n",
    "\n",
    "        The following operations are performed:\n",
    "        - Loading edges, nodes, and labels.\n",
    "        - Reading data with a specified number of hops.\n",
    "        - Updating node features and edge weights.\n",
    "        - Deleting all data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define file paths \n",
    "        node_file = f'E:/Master Thesis/Thesis_Code/data/X_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "        edge_file = f'E:/Master Thesis/Thesis_Code/data/edge_index_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "        label_file = f'E:/Master Thesis/Thesis_Code/data/y_{nodes_count}_nodes_{edges_count}_edges.csv'\n",
    "\n",
    "        nodes_count, edges_count = extract_file_info(node_file)\n",
    "        db_name = f\"db_{nodes_count}_nodes_{edges_count}_edges\"\n",
    "\n",
    "        # Create a new database\n",
    "        create_database(db_name)    \n",
    "\n",
    "        # Connect to the newly created database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        \n",
    "        # Create tables with indexing\n",
    "        create_tables_with_index(connection)\n",
    "\n",
    "        # Load nodes, edges, and labels\n",
    "        load_nodes(connection, node_file)\n",
    "        load_edges(connection, edge_file)\n",
    "        load_labels(connection, label_file)\n",
    "    \n",
    "        # Get node features and edge table\n",
    "        node_features = get_node_table_from_sql(connection)\n",
    "        print(\"Node Features Shape:\", node_features.shape)\n",
    "\n",
    "        # Load the labels from the labels table if needed\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT label FROM labels\")\n",
    "        labels_data = cursor.fetchall()\n",
    "\n",
    "        # Extract labels from fetched data\n",
    "        node_labels = np.array([int(label[0]) for label in labels_data])  # Use float() if needed\n",
    "        print(\"Node Labels Shape:\", node_labels.shape)\n",
    "\n",
    "        edge_data = get_edge_table_from_sql(connection)\n",
    "        edge_index = np.array(edge_data, dtype=np.int64).T  # Ensure edge_index is of type int64\n",
    "        print(\"Edge Data Length:\", len(edge_index))\n",
    "\n",
    "        # Close the connection\n",
    "        connection.close()\n",
    "\n",
    "        \n",
    "        return node_features, node_labels, edge_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e67b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppi_graph():\n",
    "    \n",
    "    # Imprort required libraries\n",
    "    import psycopg2\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    import re\n",
    "    import time\n",
    "    from psycopg2 import sql\n",
    "\n",
    "    # Helper function to connect to SQL database\n",
    "    def sql_connector(dataset_name):\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "\n",
    "        return connection\n",
    "       \n",
    "    # Function to create a database\n",
    "    def create_database(db_name):\n",
    "        \"\"\"\n",
    "        Creates a new PostgreSQL database.\n",
    "\n",
    "        Parameters:\n",
    "        db_name (str): The name of the database to be created.\n",
    "        \"\"\"\n",
    "\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='postgres',  # Connect to default db to create new db\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        connection.autocommit = True\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        try:\n",
    "            # Create the database\n",
    "            cursor.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(db_name)))\n",
    "            print(f\"Database {db_name} created successfully.\")\n",
    "            \n",
    "            # Connect to the newly created database and enable pg_stat_statements\n",
    "            new_db_connection = psycopg2.connect(\n",
    "                dbname=db_name,\n",
    "                user='postgres',\n",
    "                password='Berlin!321',\n",
    "                host='localhost'\n",
    "            )\n",
    "            new_db_cursor = new_db_connection.cursor()\n",
    "            \n",
    "            new_db_cursor.close()\n",
    "            new_db_connection.close()\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    # Function to create tables in the PostgreSQL database with indexing and track index creation times\n",
    "    def create_tables_with_index(connection):\n",
    "        \"\"\"\n",
    "        Creates the tables 'nodes', 'edges', and 'labels' in the connected PostgreSQL database, if they do not exist.\n",
    "        Also adds indexes on certain columns to improve query performance.\n",
    "\n",
    "        Parameters:\n",
    "            connection (PostgreSQL connection object): Connection to the PostgreSQL database.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two values:\n",
    "                - mean_data_loading_index_time (float): Mean execution time for creating the indexes.\n",
    "                - std_data_loading_index_time (float): Standard deviation of the execution time for creating the indexes.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Function to execute an index creation and track the time\n",
    "        def create_index_and_time(query):\n",
    "            cursor.execute(query)     # Execute index creation\n",
    "            connection.commit()       # Commit after each index creation\n",
    "            \n",
    "        # Create the edges table and its indexes\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS edges (\n",
    "                source INT, \n",
    "                target INT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes and track their creation times\n",
    "        create_index_and_time(\"CREATE INDEX IF NOT EXISTS source_index ON edges (source)\")\n",
    "        create_index_and_time(\"CREATE INDEX IF NOT EXISTS target_index ON edges (target)\")\n",
    "        create_index_and_time(\"CREATE INDEX IF NOT EXISTS source_target_index ON edges (source, target)\")\n",
    "        create_index_and_time(\"CREATE INDEX IF NOT EXISTS target_source_index ON edges (target, source)\")\n",
    "\n",
    "        # Create the nodes table and its features\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS nodes (node_id SERIAL PRIMARY KEY)\")\n",
    "        \n",
    "        # Dynamically add features if they do not exist\n",
    "        cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='nodes'\")\n",
    "        columns = [column[0] for column in cursor.fetchall()]\n",
    "        \n",
    "        for i in range(100):  # Assuming 100 features\n",
    "            feature_name = f\"feature_{i}\"\n",
    "            if feature_name not in columns:\n",
    "                cursor.execute(f\"ALTER TABLE nodes ADD COLUMN {feature_name} FLOAT\")\n",
    "\n",
    "        # Create the labels table and its index\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS labels (\n",
    "                node_id INT REFERENCES nodes(node_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "        create_index_and_time(\"CREATE INDEX IF NOT EXISTS node_id_index ON labels (node_id)\")\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    # Function to load edges data into PostgreSQL in batches\n",
    "    def load_edges(connection, file_path, batch_size=30000):\n",
    "        \"\"\"\n",
    "        Loads edges from a CSV file into the edges table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        edge_file (str): The path to the CSV file containing edge data.\n",
    "\n",
    "        Returns:\n",
    "        int: The number of edges loaded into the database.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Edges are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        with open(file_path, 'r') as edges_file:\n",
    "            edges_reader = csv.reader(edges_file)\n",
    "            next(edges_reader)  # Skip header\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            for row in edges_reader:\n",
    "                batch.append((int(row[0]), int(row[1])))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\") for row in batch)\n",
    "                    query = f\"INSERT INTO edges (source, target) VALUES {args_str};\"\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "\n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(cursor.mogrify(\"(%s,%s)\", row).decode(\"utf-8\") for row in batch)\n",
    "                query = f\"INSERT INTO edges (source, target) VALUES {args_str};\"\n",
    "                cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                explain_output = cursor.fetchall()\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Edge loading completed!\")\n",
    "\n",
    "    # Function to load nodes data into PostgreSQL in batches\n",
    "    def load_nodes(connection, file_path, batch_size=30000):\n",
    "        \"\"\"\n",
    "        Loads nodes from a CSV file into the nodes table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        node_file (str): The path to the CSV file containing node data.\n",
    "\n",
    "        Returns:\n",
    "        int: The number of nodes loaded into the database.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Nodes are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        with open(file_path, 'r') as nodes_file:\n",
    "            nodes_reader = csv.reader(nodes_file)\n",
    "            next(nodes_reader)  # Skip header\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            for row in nodes_reader:\n",
    "                batch.append(tuple(map(float, row)))\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(\n",
    "                        cursor.mogrify(\"(\" + \",\".join([\"%s\"] * 50) + \")\", row).decode(\"utf-8\")\n",
    "                        for row in batch\n",
    "                    )\n",
    "                    query = f\"INSERT INTO nodes ({', '.join([f'feature_{i}' for i in range(50)])}) VALUES {args_str};\"\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "\n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(\n",
    "                    cursor.mogrify(\"(\" + \",\".join([\"%s\"] * 50) + \")\", row).decode(\"utf-8\")\n",
    "                    for row in batch\n",
    "                )\n",
    "                query = f\"INSERT INTO nodes ({', '.join([f'feature_{i}' for i in range(50)])}) VALUES {args_str};\"\n",
    "                cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                explain_output = cursor.fetchall()\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Node loading completed!\")\n",
    "\n",
    "    # Function to load labels data into PostgreSQL in batches\n",
    "    def load_labels(connection, file_path, batch_size=30000):\n",
    "        \"\"\"\n",
    "        Loads labels from a CSV file into the labels table in the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "        label_file (str): The path to the CSV file containing label data.\n",
    "\n",
    "        Returns:\n",
    "        int: The number of labels loaded into the database.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Labels are loading!\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        with open(file_path, 'r') as labels_file:\n",
    "            labels_reader = csv.reader(labels_file)\n",
    "            headers = next(labels_reader)  # Get headers from the file (all label columns)\n",
    "            num_labels = len(headers)  # Number of labels is equal to the number of columns\n",
    "\n",
    "            # Ensure the label columns exist and create them if they don't\n",
    "            existing_columns = set()\n",
    "            cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name='labels'\")\n",
    "            existing_columns.update([column[0] for column in cursor.fetchall()])\n",
    "\n",
    "            for i in range(1, num_labels + 1):\n",
    "                label_column = f\"label_{i}\"\n",
    "                if label_column not in existing_columns:\n",
    "                    cursor.execute(f\"ALTER TABLE labels ADD COLUMN {label_column} TEXT\")\n",
    "\n",
    "            # Prepare SQL query for batch insertion\n",
    "            columns = \",\".join([f\"label_{i}\" for i in range(1, num_labels + 1)])\n",
    "            query_template = f\"INSERT INTO labels (node_id, {columns}) VALUES %s\"\n",
    "            \n",
    "            batch = []\n",
    "\n",
    "            for idx, row in enumerate(labels_reader, start=1):\n",
    "                node_id = idx  # Use the row index as the node_id\n",
    "                label_data = tuple(row)  # All columns are the label data\n",
    "                batch.append((node_id, *label_data))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    # Process the batch\n",
    "                    args_str = ','.join(cursor.mogrify(f\"(%s, {','.join(['%s']*num_labels)})\", row).decode(\"utf-8\") \n",
    "                                        for row in batch)\n",
    "                    query = query_template % args_str\n",
    "                    cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                    explain_output = cursor.fetchall()\n",
    "                    \n",
    "                    batch = []  # Clear batch\n",
    "\n",
    "            # Insert any remaining rows\n",
    "            if batch:\n",
    "                args_str = ','.join(cursor.mogrify(f\"(%s, {','.join(['%s']*num_labels)})\", row).decode(\"utf-8\") \n",
    "                                    for row in batch)\n",
    "                query = query_template % args_str\n",
    "                cursor.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "                explain_output = cursor.fetchall()\n",
    "\n",
    "\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        print(\"Label loading completed!\")\n",
    "\n",
    "\n",
    "    # Function to get the node table from PostgreSQL\n",
    "    def get_node_table_from_sql(dataset_name) -> np.array:\n",
    "        \"\"\"\n",
    "        Connects to the database, retrieves node features (excluding node_id) from the nodes table, \n",
    "        and returns them as a numpy array.\n",
    "        \n",
    "        Parameters:\n",
    "        dataset_name (str): The name of the dataset (i.e., the database name).\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: A 2D numpy array of node features.\n",
    "        \"\"\"\n",
    "        # Connect to the PostgreSQL database\n",
    "        connector = sql_connector(dataset_name)\n",
    "        \n",
    "        # Create a cursor from the connection\n",
    "        cursor = connector.cursor()\n",
    "        \n",
    "        # Execute the SQL query\n",
    "        cursor.execute(\"SELECT * FROM nodes\")  # Select all columns\n",
    "        node_data = cursor.fetchall()  # Fetch all the rows returned by the query\n",
    "        \n",
    "        # Close the cursor\n",
    "        cursor.close()\n",
    "\n",
    "        # Convert the data to a numpy array\n",
    "        node_data = np.array(node_data, dtype=np.float32)  # Ensure dtype is float32\n",
    "\n",
    "        # Debugging: Print the shape of the node_data\n",
    "        print(\"Fetched node data shape:\", node_data.shape)\n",
    "\n",
    "        # Ensure the node data contains the correct number of feature columns\n",
    "        if node_data.size == 0:  # Check if no data is returned\n",
    "            raise ValueError(\"No data returned from the nodes table.\")\n",
    "\n",
    "        if node_data.shape[1] != 101:  # 100 features + 1 node_id column\n",
    "            raise ValueError(f\"Unexpected data shape. Ensure node table contains exactly 100 feature columns plus the node_id. Current shape: {node_data.shape}.\")\n",
    "\n",
    "        # Extract features excluding the node_id column\n",
    "        node_features = node_data[:, 1:]  # This gets all columns except the first one (node_id)\n",
    "\n",
    "        return node_features.astype(np.float32)  # Ensure it's returned as float32\n",
    "    \n",
    "        \n",
    "    # Function to get the edge index table from PostgreSQL\n",
    "    def get_edge_table_from_sql(connection):\n",
    "        \"\"\"\n",
    "        Retrieves the edge table from the database.\n",
    "\n",
    "        Parameters:\n",
    "        connection (psycopg2.extensions.connection): The database connection.\n",
    "\n",
    "        Returns:\n",
    "        np.array: A numpy array of edges as tuples of integers.\n",
    "        \"\"\"\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT * FROM edges\")\n",
    "        edge_data = cursor.fetchall()\n",
    "\n",
    "        if not edge_data:\n",
    "            raise ValueError(\"No data returned from the edges table.\")\n",
    "\n",
    "        # Convert edge data to numpy array and ensure it's dtype int64\n",
    "        edge_data = np.array(edge_data, dtype=np.int64)\n",
    "\n",
    "        return edge_data.T  # Return in transposed form (2 x num_edges)\n",
    "\n",
    "\n",
    "    #  Main block\n",
    "    if __name__ == \"__main__\":\n",
    "        \"\"\"\n",
    "        Main execution block of the script.\n",
    "\n",
    "        This block initializes the parameters, creates the database and tables,\n",
    "        loads data from CSV files, measures performance for various operations,\n",
    "        and logs the results.\n",
    "\n",
    "        The following operations are performed:\n",
    "        - Loading edges, nodes, and labels.\n",
    "        - Reading data with a specified number of hops.\n",
    "        - Updating node features and edge weights.\n",
    "        - Deleting all data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define file paths and database    \n",
    "        edge_file = 'E:/Master Thesis/Thesis_Code/data/PPI_edge_index.csv'\n",
    "        node_file = 'E:/Master Thesis/Thesis_Code/data/PPI_X.csv'\n",
    "        label_file = 'E:/Master Thesis/Thesis_Code/data/PPI_y.csv'  \n",
    "\n",
    "        db_name = f\"query_ops_PPI\"\n",
    "        dataset_name = f\"dataset_PPI\"\n",
    "\n",
    "        # Create a new database\n",
    "        create_database(db_name)    \n",
    "\n",
    "        # Connect to the newly created database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user='postgres',\n",
    "            password='Berlin!321',\n",
    "            host='localhost'\n",
    "        )\n",
    "        \n",
    "        # Create tables with indexing\n",
    "        create_tables_with_index(connection)\n",
    "\n",
    "        # Load nodes, edges, and labels\n",
    "        load_nodes(connection, node_file)\n",
    "        load_edges(connection, edge_file)\n",
    "        load_labels(connection, label_file)\n",
    "\n",
    "        # Get node features and edge table\n",
    "        node_features = get_node_table_from_sql(connection)\n",
    "        print(\"Node Features Shape:\", node_features.shape)\n",
    "\n",
    "        # # Load the labels from the labels table if needed\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT * FROM labels\")\n",
    "        labels_data = cursor.fetchall()\n",
    "\n",
    "        # Extract labels from fetched data\n",
    "        # node_labels = np.array([label[0] for label in labels_data])  # Assuming label is in the first column\n",
    "        node_labels = np.array([row[1:] for row in labels_data], dtype=np.float32)  \n",
    "        print(\"Node Labels Shape:\", node_labels.shape)\n",
    "        \n",
    "        edge_index = get_edge_table_from_sql(connection)\n",
    "        print(\"Edge Data Length:\", len(edge_index))\n",
    "\n",
    "        # Close the connection\n",
    "        connection.close()\n",
    "\n",
    "        return node_features, node_labels, edge_index\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10375c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node and edge counts for the dataset\n",
    "nodes_count = 1000  # Change this as needed\n",
    "edges_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbc57fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database query_ops_PPI created successfully.\n",
      "Nodes are loading!\n",
      "Node loading completed!\n",
      "Edges are loading!\n",
      "Edge loading completed!\n",
      "Labels are loading!\n",
      "Label loading completed!\n",
      "Fetched node data shape: (44906, 101)\n",
      "Node Features Shape: (44906, 100)\n",
      "Node Labels Shape: (44906, 121)\n",
      "Edge Data Length: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[44906, 100], edge_index=[2, 1226368], y=[44906, 121])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the graph data using get_graph\n",
    "node_features, node_labels, edge_index = get_graph(nodes_count, edges_count)\n",
    "# node_features, node_labels, edge_index = get_graph_batches(nodes_count, edges_count)\n",
    "# node_features, node_labels, edge_index = get_ppi_graph()\n",
    "\n",
    "# # Print the types and shapes to inspect the arrays\n",
    "# print(f\"Node Features: {type(node_features)}, dtype: {node_features.dtype}, shape: {node_features.shape}\")\n",
    "# print(f\"Node Labels: {type(node_labels)}, dtype: {node_labels.dtype}, shape: {node_labels.shape}\")\n",
    "# print(f\"Edge Index: {type(edge_index)}, dtype: {edge_index.dtype}, shape: {edge_index.shape}\")\n",
    "\n",
    "\n",
    "data = Data(x = torch.from_numpy(node_features).type(torch.float32), y = torch.from_numpy(node_labels).type(torch.float32), edge_index = torch.from_numpy(edge_index).type(torch.long))\n",
    "data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31534942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (in_gat): GATConv(100, 256, heads=8)\n",
       "  (hidden_gat): GATConv(2048, 256, heads=8)\n",
       "  (out_gat): GATConv(2048, 1, heads=8)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn.conv import GATConv\n",
    "from memory_profiler import profile\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, heads = 8):\n",
    "        super(GNN, self).__init__()\n",
    "        self.in_gat = GATConv(in_dim, hidden_dim, heads = heads, concat=True)\n",
    "        self.hidden_gat = GATConv(hidden_dim*8, hidden_dim, heads = heads, concat=True)\n",
    "        self.out_gat = GATConv(hidden_dim*8, out_dim, heads = heads, concat=False)\n",
    "\n",
    "    @profile\n",
    "    def forward(self,graph):\n",
    "        x, edge_index = graph.x, graph.edge_index\n",
    "        x = torch.relu(self.in_gat(x, edge_index))\n",
    "        x = torch.relu(self.hidden_gat(x, edge_index))\n",
    "        x = self.out_gat(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN(100, 256, 1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b072273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_8924/2240319332.py\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 10414276608 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8924/1253525023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmem_usage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_forward_pass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Peak memory usage: {max(mem_usage)}MiB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[1;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0mreturned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# finish timing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8924/1253525023.py\u001b[0m in \u001b[0;36mrun_forward_pass\u001b[1;34m(model, input_data)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmem_usage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_forward_pass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m                 \u001b[0mprof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m                 \u001b[0mshow_results_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\memory_profiler.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_ctxmgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8924/2240319332.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_gat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_gat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_gat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_propagate_k1404awa.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, x, alpha, size)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# End Message Forward Pre Hook #########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         out = self.message(\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mx_j\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\Anaconda\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[1;34m(self, x_j, alpha)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_j\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 10414276608 bytes."
     ]
    }
   ],
   "source": [
    "# from torch_geometric.data import Data\n",
    "\n",
    "# data = Data(x = x, edge_index= edge_index)\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "def run_forward_pass(model, input_data):\n",
    "    return model(input_data)\n",
    "\n",
    "mem_usage = memory_usage(proc=(run_forward_pass, (model, data), {}))\n",
    "print(f\"Peak memory usage: {max(mem_usage)}MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
